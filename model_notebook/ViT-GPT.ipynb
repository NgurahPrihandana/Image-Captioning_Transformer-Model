{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10112445,"sourceType":"datasetVersion","datasetId":6238855}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Import Libraries\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom nltk.translate.bleu_score import corpus_bleu\n# from nltk.translate.meteor_score import single_meteor_score\nfrom gensim.models import KeyedVectors\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision\n\nfrom transformers import AutoTokenizer\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom tqdm.notebook import trange, tqdm\n\nfrom torch.distributions import Categorical\n\ntorch.backends.cuda.matmul.allow_tf32 = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:29:52.240216Z","iopub.execute_input":"2024-12-18T17:29:52.240564Z","iopub.status.idle":"2024-12-18T17:30:09.79388Z","shell.execute_reply.started":"2024-12-18T17:29:52.240531Z","shell.execute_reply":"2024-12-18T17:30:09.792915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nltk\nfrom nltk.corpus import wordnet\n\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:09.79569Z","iopub.execute_input":"2024-12-18T17:30:09.796539Z","iopub.status.idle":"2024-12-18T17:30:11.167283Z","shell.execute_reply.started":"2024-12-18T17:30:09.796493Z","shell.execute_reply":"2024-12-18T17:30:11.166346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U nltk rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:11.16857Z","iopub.execute_input":"2024-12-18T17:30:11.168838Z","iopub.status.idle":"2024-12-18T17:30:24.802105Z","shell.execute_reply.started":"2024-12-18T17:30:11.168812Z","shell.execute_reply":"2024-12-18T17:30:24.80109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nfrom rouge_score import rouge_scorer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:24.805288Z","iopub.execute_input":"2024-12-18T17:30:24.805735Z","iopub.status.idle":"2024-12-18T17:30:24.862509Z","shell.execute_reply.started":"2024-12-18T17:30:24.805688Z","shell.execute_reply":"2024-12-18T17:30:24.861691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:24.863421Z","iopub.execute_input":"2024-12-18T17:30:24.863662Z","iopub.status.idle":"2024-12-18T17:30:33.302498Z","shell.execute_reply.started":"2024-12-18T17:30:24.86364Z","shell.execute_reply":"2024-12-18T17:30:33.301331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import ViTModel, GPT2LMHeadModel, GPT2Config, VisionEncoderDecoderModel, ViTFeatureExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:33.30423Z","iopub.execute_input":"2024-12-18T17:30:33.304645Z","iopub.status.idle":"2024-12-18T17:30:45.629244Z","shell.execute_reply.started":"2024-12-18T17:30:33.304607Z","shell.execute_reply":"2024-12-18T17:30:45.628542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Load Dataset","metadata":{}},{"cell_type":"code","source":"vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:45.630252Z","iopub.execute_input":"2024-12-18T17:30:45.630765Z","iopub.status.idle":"2024-12-18T17:30:55.415389Z","shell.execute_reply.started":"2024-12-18T17:30:45.630737Z","shell.execute_reply":"2024-12-18T17:30:55.414522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.416578Z","iopub.execute_input":"2024-12-18T17:30:55.416895Z","iopub.status.idle":"2024-12-18T17:30:55.789752Z","shell.execute_reply.started":"2024-12-18T17:30:55.416836Z","shell.execute_reply":"2024-12-18T17:30:55.788924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"work_directory = \"/kaggle/input/deep-learning-ic-dataset/\"\ndata_path = os.path.join(work_directory, \"captions.csv\")\ndata = pd.read_csv(data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.790774Z","iopub.execute_input":"2024-12-18T17:30:55.791099Z","iopub.status.idle":"2024-12-18T17:30:55.85064Z","shell.execute_reply.started":"2024-12-18T17:30:55.791072Z","shell.execute_reply":"2024-12-18T17:30:55.849986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\ntemp_directory = Path('../temp')\ntemp_directory.mkdir(exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.854405Z","iopub.execute_input":"2024-12-18T17:30:55.855263Z","iopub.status.idle":"2024-12-18T17:30:55.859954Z","shell.execute_reply.started":"2024-12-18T17:30:55.855183Z","shell.execute_reply":"2024-12-18T17:30:55.858933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.861076Z","iopub.execute_input":"2024-12-18T17:30:55.86132Z","iopub.status.idle":"2024-12-18T17:30:55.884733Z","shell.execute_reply.started":"2024-12-18T17:30:55.861295Z","shell.execute_reply":"2024-12-18T17:30:55.883983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['filepath'] = data['filepath'].apply(lambda x: os.path.join(work_directory, x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.885947Z","iopub.execute_input":"2024-12-18T17:30:55.886217Z","iopub.status.idle":"2024-12-18T17:30:55.908744Z","shell.execute_reply.started":"2024-12-18T17:30:55.886191Z","shell.execute_reply":"2024-12-18T17:30:55.9079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.91003Z","iopub.execute_input":"2024-12-18T17:30:55.910798Z","iopub.status.idle":"2024-12-18T17:30:55.929527Z","shell.execute_reply.started":"2024-12-18T17:30:55.910759Z","shell.execute_reply":"2024-12-18T17:30:55.928706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the split datasets for easier access later (optional)\ntrain_csv_path = os.path.join(temp_directory, \"train_captions.csv\")\nval_csv_path = os.path.join(temp_directory, \"val_captions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.930546Z","iopub.execute_input":"2024-12-18T17:30:55.930789Z","iopub.status.idle":"2024-12-18T17:30:55.944624Z","shell.execute_reply.started":"2024-12-18T17:30:55.930766Z","shell.execute_reply":"2024-12-18T17:30:55.943896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.to_csv(train_csv_path, index=False)\nval_data.to_csv(val_csv_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:55.945568Z","iopub.execute_input":"2024-12-18T17:30:55.945827Z","iopub.status.idle":"2024-12-18T17:30:56.020689Z","shell.execute_reply.started":"2024-12-18T17:30:55.945804Z","shell.execute_reply":"2024-12-18T17:30:56.019872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomImageCaptionDataset(Dataset):\n    def __init__(self, data_frame, transform=None):\n        self.data = data_frame\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Get file path and caption\n        image_info = self.data.iloc[idx]\n        image_path = image_info['filepath']  # Use the filepath column directly\n        caption = image_info['caption']\n\n        # Load and transform the image\n        image = Image.open(image_path).convert('RGB')  # Pastikan format PIL.Image\n       \n        return image, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:56.021657Z","iopub.execute_input":"2024-12-18T17:30:56.021929Z","iopub.status.idle":"2024-12-18T17:30:56.0272Z","shell.execute_reply.started":"2024-12-18T17:30:56.021905Z","shell.execute_reply":"2024-12-18T17:30:56.026328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_size = 128\nbatch_size = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:56.028463Z","iopub.execute_input":"2024-12-18T17:30:56.029067Z","iopub.status.idle":"2024-12-18T17:30:56.038976Z","shell.execute_reply.started":"2024-12-18T17:30:56.029029Z","shell.execute_reply":"2024-12-18T17:30:56.038106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = CustomImageCaptionDataset(\n    data_frame=train_data,\n\n)\n\nval_dataset = CustomImageCaptionDataset(\n    data_frame=val_data,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:56.040061Z","iopub.execute_input":"2024-12-18T17:30:56.040317Z","iopub.status.idle":"2024-12-18T17:30:56.05066Z","shell.execute_reply.started":"2024-12-18T17:30:56.040292Z","shell.execute_reply":"2024-12-18T17:30:56.049889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We'll use a pre-built Tokenizer for the BERT Model\n# https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:56.05164Z","iopub.execute_input":"2024-12-18T17:30:56.051912Z","iopub.status.idle":"2024-12-18T17:30:58.322859Z","shell.execute_reply.started":"2024-12-18T17:30:56.051886Z","shell.execute_reply":"2024-12-18T17:30:58.322068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    images, captions = zip(*batch)\n    # Tokenize dynamically\n    images, captions = zip(*batch)\n    return images, list(captions)  # Return captions as a list of raw strings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.323963Z","iopub.execute_input":"2024-12-18T17:30:58.324219Z","iopub.status.idle":"2024-12-18T17:30:58.328773Z","shell.execute_reply.started":"2024-12-18T17:30:58.324194Z","shell.execute_reply":"2024-12-18T17:30:58.327865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Loaders\ndata_loader_train = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n)\n\ndata_loader_val = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.329918Z","iopub.execute_input":"2024-12-18T17:30:58.330203Z","iopub.status.idle":"2024-12-18T17:30:58.352298Z","shell.execute_reply.started":"2024-12-18T17:30:58.330178Z","shell.execute_reply":"2024-12-18T17:30:58.351516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataiter = next(iter(data_loader_val))\ntest_images, test_captions = dataiter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.353331Z","iopub.execute_input":"2024-12-18T17:30:58.353661Z","iopub.status.idle":"2024-12-18T17:30:58.637726Z","shell.execute_reply.started":"2024-12-18T17:30:58.353621Z","shell.execute_reply":"2024-12-18T17:30:58.63706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the PIL images to tensors using ViTFeatureExtractor\ninputs = feature_extractor(images=[test_images[1]], return_tensors=\"pt\")  # Process a single image\n\n# Extract the image tensor\nimage_tensor = inputs[\"pixel_values\"][0]  # Shape: [C, H, W]\n\n# Visualize the image\nplt.figure(figsize=(3, 3))\nplt.imshow(image_tensor.permute(1, 2, 0).numpy())  # Permute to [H, W, C] for visualization\nplt.show()\n\n# Print the corresponding caption\ncaption = test_captions[1]\nprint(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.638682Z","iopub.execute_input":"2024-12-18T17:30:58.639036Z","iopub.status.idle":"2024-12-18T17:30:58.886955Z","shell.execute_reply.started":"2024-12-18T17:30:58.639Z","shell.execute_reply":"2024-12-18T17:30:58.886039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3: Start Modelling","metadata":{}},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.887995Z","iopub.execute_input":"2024-12-18T17:30:58.888265Z","iopub.status.idle":"2024-12-18T17:30:58.893741Z","shell.execute_reply.started":"2024-12-18T17:30:58.888239Z","shell.execute_reply":"2024-12-18T17:30:58.89288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokens = tokenizer(test_captions, padding=True, truncation=True, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.894845Z","iopub.execute_input":"2024-12-18T17:30:58.895233Z","iopub.status.idle":"2024-12-18T17:30:58.909784Z","shell.execute_reply.started":"2024-12-18T17:30:58.895204Z","shell.execute_reply":"2024-12-18T17:30:58.909129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokens['attention_mask']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.910751Z","iopub.execute_input":"2024-12-18T17:30:58.911078Z","iopub.status.idle":"2024-12-18T17:30:58.925252Z","shell.execute_reply.started":"2024-12-18T17:30:58.911052Z","shell.execute_reply":"2024-12-18T17:30:58.924455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_ids = tokens['input_ids'][0]\ntokens['input_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.92638Z","iopub.execute_input":"2024-12-18T17:30:58.927216Z","iopub.status.idle":"2024-12-18T17:30:58.940368Z","shell.execute_reply.started":"2024-12-18T17:30:58.927174Z","shell.execute_reply":"2024-12-18T17:30:58.939503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Prepare a GPT-2 decoder configuration that includes cross-attention:\n\ndecoder_config = GPT2Config.from_pretrained(\"gpt2\")\ndecoder_config.add_cross_attention = True  # This is crucial\ngpt2_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=decoder_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:30:58.944347Z","iopub.execute_input":"2024-12-18T17:30:58.944624Z","iopub.status.idle":"2024-12-18T17:31:13.255332Z","shell.execute_reply.started":"2024-12-18T17:30:58.944599Z","shell.execute_reply":"2024-12-18T17:31:13.254686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = VisionEncoderDecoderModel(encoder=vit_model, decoder=gpt2_decoder)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:31:13.25642Z","iopub.execute_input":"2024-12-18T17:31:13.256688Z","iopub.status.idle":"2024-12-18T17:31:13.263482Z","shell.execute_reply.started":"2024-12-18T17:31:13.256661Z","shell.execute_reply":"2024-12-18T17:31:13.26264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"tokenizer setup","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2TokenizerFast\n\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a real pad token, use eos\n\n#Configure model start and end tokens:\n\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:31:13.264489Z","iopub.execute_input":"2024-12-18T17:31:13.26477Z","iopub.status.idle":"2024-12-18T17:31:15.742206Z","shell.execute_reply.started":"2024-12-18T17:31:13.264719Z","shell.execute_reply":"2024-12-18T17:31:15.741402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom tqdm.notebook import trange, tqdm\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nfrom rouge_score import rouge_scorer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnepochs = 50\ntraining_loss_logger = []\neval_loss_logger = []\neval_bleu_logger = []\neval_meteor_logger = []\neval_rouge_logger = []\n\nmax_length = 25  # Adjust if needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:31:15.743267Z","iopub.execute_input":"2024-12-18T17:31:15.743548Z","iopub.status.idle":"2024-12-18T17:31:16.345369Z","shell.execute_reply.started":"2024-12-18T17:31:15.743522Z","shell.execute_reply":"2024-12-18T17:31:16.344591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the EarlyStopping Class\nclass EarlyStopping:\n    def __init__(self, patience=2, verbose=False, delta=0.0, path='best_model.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.delta = delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.path = path\n        self.best_model_state = None\n\n    def __call__(self, current_loss, model):\n        if self.best_loss is None:\n            self.best_loss = current_loss\n            self.save_checkpoint(model)\n        elif current_loss > self.best_loss - self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = current_loss\n            self.save_checkpoint(model)\n            self.counter = 0\n\n    def save_checkpoint(self, model):\n        \"\"\"Saves model when validation loss decreases.\"\"\"\n        torch.save(model.state_dict(), self.path)\n        if self.verbose:\n            print(f'Validation loss decreased. Saving model to {self.path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:31:16.346431Z","iopub.execute_input":"2024-12-18T17:31:16.346716Z","iopub.status.idle":"2024-12-18T17:31:16.353688Z","shell.execute_reply.started":"2024-12-18T17:31:16.346689Z","shell.execute_reply":"2024-12-18T17:31:16.35281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Early Stopping with model checkpoint path\nearly_stopping = EarlyStopping(patience=5, verbose=True, path='best_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:31:16.354683Z","iopub.execute_input":"2024-12-18T17:31:16.35499Z","iopub.status.idle":"2024-12-18T17:31:16.370079Z","shell.execute_reply.started":"2024-12-18T17:31:16.354951Z","shell.execute_reply":"2024-12-18T17:31:16.369035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n\n# Hide all warnings\nwarnings.filterwarnings('ignore')\n\n# OR, to hide only a specific warning message, you can do something like:\n# warnings.filterwarnings('ignore', message=\"The attention mask and the pad token id were not set.*\")\n\n# Rest of your code","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:31:16.371228Z","iopub.execute_input":"2024-12-18T17:31:16.37149Z","iopub.status.idle":"2024-12-18T17:31:16.384559Z","shell.execute_reply.started":"2024-12-18T17:31:16.371465Z","shell.execute_reply":"2024-12-18T17:31:16.383887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# See how many Parameters our Model has!\nnum_model_params = 0\nfor param in model.parameters():\n    num_model_params += param.flatten().shape[0]\n\nprint(\"Number of Model Parameters : %d or >%d Juta Params!\" % (num_model_params, num_model_params//1e6))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nrouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nfor epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n    # Training phase\n    model.train()\n    epoch_train_loss = 0.0\n    num_train_batches = 0\n    \n    for images, captions in tqdm(data_loader_train, desc=\"Training\", leave=False):\n        # Preprocess images\n        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n        pixel_values = inputs[\"pixel_values\"]\n        \n        # Tokenize captions\n        tokenized = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n        \n        # Forward pass: Model computes loss when labels are provided\n        outputs = model(pixel_values=pixel_values, labels=tokenized.input_ids)\n        loss = outputs.loss\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        epoch_train_loss += loss.item()\n        num_train_batches += 1\n\n    avg_train_loss = epoch_train_loss / num_train_batches\n    training_loss_logger.append(avg_train_loss)\n\n    # Evaluation phase\n    model.eval()\n    epoch_eval_loss = 0.0\n    num_eval_batches = 0\n    bleu_scores = []\n    meteor_scores = []\n    rouge1_scores = []\n    rouge2_scores = []\n    rougel_scores = []\n\n    with torch.no_grad():\n        for images, captions in tqdm(data_loader_val, desc=\"Eval\", leave=False):\n            inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n            pixel_values = inputs[\"pixel_values\"]\n\n            tokenized = tokenizer(captions, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n\n            # Compute validation loss\n            outputs = model(pixel_values=pixel_values, labels=tokenized.input_ids)\n            val_loss = outputs.loss.item()\n            epoch_eval_loss += val_loss\n            num_eval_batches += 1\n\n            # Generate predictions\n            pred_ids = model.generate(pixel_values=pixel_values, max_length=max_length)\n            predicted_captions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n\n            # Compute metrics\n            for pred_caption, gt_caption in zip(predicted_captions, captions):\n                reference = [tokenizer.tokenize(gt_caption.lower())]\n                hypothesis = tokenizer.tokenize(pred_caption.lower())\n\n                # BLEU Score\n                smoothing_fn = SmoothingFunction().method1\n                bleu = sentence_bleu(reference, hypothesis, smoothing_function=smoothing_fn)\n                bleu_scores.append(bleu)\n\n                # METEOR Score\n                meteor = meteor_score(reference, hypothesis)\n                meteor_scores.append(meteor)\n\n                # ROUGE Scores\n                scores = rouge.score(gt_caption, pred_caption)\n                rouge1_scores.append(scores['rouge1'].fmeasure)\n                rouge2_scores.append(scores['rouge2'].fmeasure)\n                rougel_scores.append(scores['rougeL'].fmeasure)\n\n    avg_eval_loss = epoch_eval_loss / num_eval_batches if num_eval_batches > 0 else 0.0\n    eval_loss_logger.append(avg_eval_loss)\n\n    avg_bleu_score = np.mean(bleu_scores) if bleu_scores else 0.0\n    avg_meteor_score = np.mean(meteor_scores) if meteor_scores else 0.0\n    avg_rouge1 = np.mean(rouge1_scores) if rouge1_scores else 0.0\n    avg_rouge2 = np.mean(rouge2_scores) if rouge2_scores else 0.0\n    avg_rougeL = np.mean(rougel_scores) if rougel_scores else 0.0\n\n    eval_bleu_logger.append(avg_bleu_score)\n    eval_meteor_logger.append(avg_meteor_score)\n    eval_rouge_logger.append({'rouge1': avg_rouge1, 'rouge2': avg_rouge2, 'rougeL': avg_rougeL})\n\n    print(f\"Epoch {epoch + 1}/{nepochs} - \"\n          f\"Avg Eval Loss: {avg_eval_loss:.4f} - \"\n          f\"Avg BLEU: {avg_bleu_score:.4f} - \"\n          f\"Avg Meteor: {avg_meteor_score:.4f} - \"\n          f\"Avg Rouge1: {avg_rouge1:.4f}, Rouge2: {avg_rouge2:.4f}, RougeL: {avg_rougeL:.4f}\")\n\n    # Early Stopping Check\n    early_stopping(avg_eval_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered. Restoring the best model.\")\n        model.load_state_dict(torch.load('best_model.pt'))\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:31:16.385841Z","iopub.execute_input":"2024-12-18T17:31:16.386283Z","iopub.status.idle":"2024-12-18T19:41:16.99794Z","shell.execute_reply.started":"2024-12-18T17:31:16.386241Z","shell.execute_reply":"2024-12-18T19:41:16.996924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model's state dict\ntorch.save(model.state_dict(), \"caption_model_state_dict.pth\")\n\n# Save model and optimizer state dicts\ntorch.save({\n    'epoch': nepochs,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': training_loss_logger[-1],\n}, \"checkpoint.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:16.999171Z","iopub.execute_input":"2024-12-18T19:41:16.999474Z","iopub.status.idle":"2024-12-18T19:41:22.376152Z","shell.execute_reply.started":"2024-12-18T19:41:16.999444Z","shell.execute_reply":"2024-12-18T19:41:22.375149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'caption_model_state_dict.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:22.377677Z","iopub.execute_input":"2024-12-18T19:41:22.377999Z","iopub.status.idle":"2024-12-18T19:41:22.385406Z","shell.execute_reply.started":"2024-12-18T19:41:22.377972Z","shell.execute_reply":"2024-12-18T19:41:22.384529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Just check the lengths of the logs\nprint(len(training_loss_logger), len(eval_loss_logger), len(eval_bleu_logger))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:22.386717Z","iopub.execute_input":"2024-12-18T19:41:22.387131Z","iopub.status.idle":"2024-12-18T19:41:23.812731Z","shell.execute_reply.started":"2024-12-18T19:41:22.387084Z","shell.execute_reply":"2024-12-18T19:41:23.811876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assume avg_training_loss_logger and avg_eval_loss_logger contain epoch-level losses\navg_training_loss_logger = training_loss_logger  # Replace with epoch-wise averaged training loss\navg_eval_loss_logger = eval_loss_logger          # Replace with epoch-wise averaged evaluation loss\n\n# Define the range and step size for y-axis ticks\ny_min, y_max = min(min(avg_training_loss_logger), min(avg_eval_loss_logger)), \\\n               max(max(avg_training_loss_logger), max(avg_eval_loss_logger))\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks (adjust if needed)\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\nplt.figure(figsize=(10, 5))\n\n# Plot average training loss per epoch\nplt.plot(range(1, len(avg_training_loss_logger) + 1), avg_training_loss_logger, \n         label=\"Training Loss\", color='tab:blue')\n\n# Plot average evaluation loss per epoch\nplt.plot(range(1, len(avg_eval_loss_logger) + 1), avg_eval_loss_logger, \n         label=\"Evaluation Loss\", color='tab:orange')\n\n# Apply custom y-ticks\nplt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\nplt.title(\"Training and Evaluation Loss per Epoch\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:23.813742Z","iopub.execute_input":"2024-12-18T19:41:23.81451Z","iopub.status.idle":"2024-12-18T19:41:25.898023Z","shell.execute_reply.started":"2024-12-18T19:41:23.814481Z","shell.execute_reply":"2024-12-18T19:41:25.897159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the range and step size for y-axis ticks\ny_min, y_max = min(eval_bleu_logger), max(eval_bleu_logger)\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks (adjust if needed)\ny_ticks = np.arange(y_min, y_max + step, step)\n\n_ = plt.figure(figsize=(10, 5))\n_ = plt.plot(eval_bleu_logger[:])\n_ = plt.title(\"Bleu Score\")\n\n# Apply custom y-ticks with fewer steps\n_ = plt.yticks(y_ticks)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:25.899116Z","iopub.execute_input":"2024-12-18T19:41:25.899445Z","iopub.status.idle":"2024-12-18T19:41:30.633449Z","shell.execute_reply.started":"2024-12-18T19:41:25.899416Z","shell.execute_reply":"2024-12-18T19:41:30.632591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the range and step size for y-axis ticks for both training and eval losses\ny_min, y_max = min(min(eval_bleu_logger), min(eval_meteor_logger)), max(max(eval_bleu_logger), max(eval_meteor_logger))\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks (adjust if needed)\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\n_ = plt.figure(figsize=(10, 5))\n\n# Plot training loss\n_ = plt.plot(eval_bleu_logger, label=\"Bleu\", color='tab:blue')\n\n# Plot evaluation loss\n_ = plt.plot(eval_meteor_logger, label=\"Meteor\", color='tab:orange')\n\n# Apply custom y-ticks\n_ = plt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\n_ = plt.title(\"Bleu and Meteor Value\")\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"Loss\")\n_ = plt.legend()\n_ = plt.grid(True)\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:30.634703Z","iopub.execute_input":"2024-12-18T19:41:30.635397Z","iopub.status.idle":"2024-12-18T19:41:32.613276Z","shell.execute_reply.started":"2024-12-18T19:41:30.635347Z","shell.execute_reply":"2024-12-18T19:41:32.612398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract ROUGE scores\nrouge1_scores = [entry['rouge1'] for entry in eval_rouge_logger]\nrouge2_scores = [entry['rouge2'] for entry in eval_rouge_logger]\nrougeL_scores = [entry['rougeL'] for entry in eval_rouge_logger]\n\n# Define the range and step size for y-axis ticks for ROUGE scores\nall_rouge_scores = rouge1_scores + rouge2_scores + rougeL_scores\ny_min, y_max = min(all_rouge_scores), max(all_rouge_scores)\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\n_ = plt.figure(figsize=(10, 5))\n\n# Plot ROUGE scores\n_ = plt.plot(rouge1_scores, label=\"ROUGE-1\", color='tab:green')\n_ = plt.plot(rouge2_scores, label=\"ROUGE-2\", color='tab:red')\n_ = plt.plot(rougeL_scores, label=\"ROUGE-L\", color='tab:purple')\n\n# Apply custom y-ticks\n_ = plt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\n_ = plt.title(\"ROUGE Scores Across Epochs\")\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"ROUGE Score\")\n_ = plt.legend()\n_ = plt.grid(True)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:32.614633Z","iopub.execute_input":"2024-12-18T19:41:32.615012Z","iopub.status.idle":"2024-12-18T19:41:34.11824Z","shell.execute_reply.started":"2024-12-18T19:41:32.614973Z","shell.execute_reply":"2024-12-18T19:41:34.117226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract ROUGE scores\nrouge1_scores = [entry['rouge1'] for entry in eval_rouge_logger]\nrouge2_scores = [entry['rouge2'] for entry in eval_rouge_logger]\nrougeL_scores = [entry['rougeL'] for entry in eval_rouge_logger]\n\n# Define the range and step size for y-axis ticks for all metrics\nall_metrics = eval_bleu_logger + eval_meteor_logger + rouge1_scores + rouge2_scores + rougeL_scores\ny_min, y_max = min(all_metrics), max(all_metrics)\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\n_ = plt.figure(figsize=(12, 6))\n\n# Plot BLEU and Meteor scores\n_ = plt.plot(eval_bleu_logger, label=\"BLEU\", color='tab:blue')\n_ = plt.plot(eval_meteor_logger, label=\"Meteor\", color='tab:orange')\n\n# Plot ROUGE scores\n_ = plt.plot(rouge1_scores, label=\"ROUGE-1\", color='tab:green')\n_ = plt.plot(rouge2_scores, label=\"ROUGE-2\", color='tab:red')\n_ = plt.plot(rougeL_scores, label=\"ROUGE-L\", color='tab:purple')\n\n# Apply custom y-ticks\n_ = plt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\n_ = plt.title(\"Evaluation Metrics Across Epochs\")\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"Metric Value\")\n_ = plt.legend()\n_ = plt.grid(True)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:34.119587Z","iopub.execute_input":"2024-12-18T19:41:34.120329Z","iopub.status.idle":"2024-12-18T19:41:36.081284Z","shell.execute_reply.started":"2024-12-18T19:41:34.120283Z","shell.execute_reply":"2024-12-18T19:41:36.080302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For inference on a new image:\ndataiter = next(iter(data_loader_val))\ntest_images, test_captions = dataiter\n\nindex = 2\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\ntest_image = transform(test_images[index]).unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:36.082923Z","iopub.execute_input":"2024-12-18T19:41:36.083352Z","iopub.status.idle":"2024-12-18T19:41:36.779614Z","shell.execute_reply.started":"2024-12-18T19:41:36.083307Z","shell.execute_reply":"2024-12-18T19:41:36.778617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(3,3))\nout = torchvision.utils.make_grid(test_image, 1, normalize=True)\n_ = plt.imshow(out.permute(1, 2, 0).numpy())\nprint(\"Ground Truth Caption:\", test_captions[index])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:36.78092Z","iopub.execute_input":"2024-12-18T19:41:36.781218Z","iopub.status.idle":"2024-12-18T19:41:37.044364Z","shell.execute_reply.started":"2024-12-18T19:41:36.78119Z","shell.execute_reply":"2024-12-18T19:41:37.043515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now to generate a caption using the model:\nmodel.eval()\nwith torch.no_grad():\n    pixel_values = test_image.to(device)\n    # If your model expects pixel_values in a specific format, ensure they match.\n    # If using the feature extractor:\n    # inputs = feature_extractor(images=[transforms.ToPILImage()(test_image.squeeze(0))], return_tensors=\"pt\").to(device)\n    # pixel_values = inputs[\"pixel_values\"]\n\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n    pred_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(\"Predicted Caption:\", pred_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:37.045372Z","iopub.execute_input":"2024-12-18T19:41:37.045641Z","iopub.status.idle":"2024-12-18T19:41:37.274928Z","shell.execute_reply.started":"2024-12-18T19:41:37.045616Z","shell.execute_reply":"2024-12-18T19:41:37.274018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model if needed\ntorch.save(model.state_dict(), \"/kaggle/working/ViT-GPT2_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T19:41:37.276077Z","iopub.execute_input":"2024-12-18T19:41:37.276371Z","iopub.status.idle":"2024-12-18T19:41:38.663555Z","shell.execute_reply.started":"2024-12-18T19:41:37.276336Z","shell.execute_reply":"2024-12-18T19:41:38.662832Z"}},"outputs":[],"execution_count":null}]}