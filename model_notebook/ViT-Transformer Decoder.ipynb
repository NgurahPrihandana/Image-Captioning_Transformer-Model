{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10112445,"sourceType":"datasetVersion","datasetId":6238855}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Import Libraries\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom nltk.translate.bleu_score import corpus_bleu\n# from nltk.translate.meteor_score import single_meteor_score\nfrom gensim.models import KeyedVectors\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision\n\nfrom transformers import AutoTokenizer\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom tqdm.notebook import trange, tqdm\n\nfrom torch.distributions import Categorical\n\ntorch.backends.cuda.matmul.allow_tf32 = True","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:02.690843Z","iopub.execute_input":"2024-12-18T12:30:02.691704Z","iopub.status.idle":"2024-12-18T12:30:19.212575Z","shell.execute_reply.started":"2024-12-18T12:30:02.69165Z","shell.execute_reply":"2024-12-18T12:30:19.211872Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nltk\nfrom nltk.corpus import wordnet\n\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:19.213927Z","iopub.execute_input":"2024-12-18T12:30:19.214346Z","iopub.status.idle":"2024-12-18T12:30:20.561111Z","shell.execute_reply.started":"2024-12-18T12:30:19.214319Z","shell.execute_reply":"2024-12-18T12:30:20.560026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U nltk rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:20.562625Z","iopub.execute_input":"2024-12-18T12:30:20.563438Z","iopub.status.idle":"2024-12-18T12:30:33.813099Z","shell.execute_reply.started":"2024-12-18T12:30:20.563388Z","shell.execute_reply":"2024-12-18T12:30:33.811907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nfrom rouge_score import rouge_scorer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:33.815989Z","iopub.execute_input":"2024-12-18T12:30:33.816749Z","iopub.status.idle":"2024-12-18T12:30:33.869611Z","shell.execute_reply.started":"2024-12-18T12:30:33.81667Z","shell.execute_reply":"2024-12-18T12:30:33.868647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:33.870612Z","iopub.execute_input":"2024-12-18T12:30:33.870894Z","iopub.status.idle":"2024-12-18T12:30:42.759745Z","shell.execute_reply.started":"2024-12-18T12:30:33.870868Z","shell.execute_reply":"2024-12-18T12:30:42.758433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import ViTModel, ViTFeatureExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:42.761505Z","iopub.execute_input":"2024-12-18T12:30:42.762292Z","iopub.status.idle":"2024-12-18T12:30:53.976531Z","shell.execute_reply.started":"2024-12-18T12:30:42.762241Z","shell.execute_reply":"2024-12-18T12:30:53.97562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Memuat pre-trained model dan feature extractor\nvit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:53.977765Z","iopub.execute_input":"2024-12-18T12:30:53.978273Z","iopub.status.idle":"2024-12-18T12:30:57.843556Z","shell.execute_reply.started":"2024-12-18T12:30:53.978244Z","shell.execute_reply":"2024-12-18T12:30:57.842764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Load Dataset\nwork_directory = \"/kaggle/input/deep-learning-ic-dataset/\"\ndata_path = os.path.join(work_directory, \"captions.csv\")\ndata = pd.read_csv(data_path)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:57.844899Z","iopub.execute_input":"2024-12-18T12:30:57.845569Z","iopub.status.idle":"2024-12-18T12:30:57.902614Z","shell.execute_reply.started":"2024-12-18T12:30:57.845527Z","shell.execute_reply":"2024-12-18T12:30:57.901923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\ntemp_directory = Path('../temp')\ntemp_directory.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:57.903808Z","iopub.execute_input":"2024-12-18T12:30:57.904197Z","iopub.status.idle":"2024-12-18T12:30:57.909102Z","shell.execute_reply.started":"2024-12-18T12:30:57.904166Z","shell.execute_reply":"2024-12-18T12:30:57.908213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:57.912034Z","iopub.execute_input":"2024-12-18T12:30:57.912336Z","iopub.status.idle":"2024-12-18T12:30:57.93036Z","shell.execute_reply.started":"2024-12-18T12:30:57.912298Z","shell.execute_reply":"2024-12-18T12:30:57.929551Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['filepath'] = data['filepath'].apply(lambda x: os.path.join(work_directory, x))","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:57.931346Z","iopub.execute_input":"2024-12-18T12:30:57.931663Z","iopub.status.idle":"2024-12-18T12:30:57.955377Z","shell.execute_reply.started":"2024-12-18T12:30:57.931632Z","shell.execute_reply":"2024-12-18T12:30:57.954627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:57.956306Z","iopub.execute_input":"2024-12-18T12:30:57.956549Z","iopub.status.idle":"2024-12-18T12:30:57.969441Z","shell.execute_reply.started":"2024-12-18T12:30:57.956525Z","shell.execute_reply":"2024-12-18T12:30:57.968772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the split datasets for easier access later (optional)\ntrain_csv_path = os.path.join(temp_directory, \"train_captions.csv\")\nval_csv_path = os.path.join(temp_directory, \"val_captions.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:57.970386Z","iopub.execute_input":"2024-12-18T12:30:57.970658Z","iopub.status.idle":"2024-12-18T12:30:57.981035Z","shell.execute_reply.started":"2024-12-18T12:30:57.970634Z","shell.execute_reply":"2024-12-18T12:30:57.980233Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.to_csv(train_csv_path, index=False)\nval_data.to_csv(val_csv_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:57.981951Z","iopub.execute_input":"2024-12-18T12:30:57.9822Z","iopub.status.idle":"2024-12-18T12:30:58.054685Z","shell.execute_reply.started":"2024-12-18T12:30:57.982175Z","shell.execute_reply":"2024-12-18T12:30:58.053857Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomImageCaptionDataset(Dataset):\n    def __init__(self, data_frame, transform=None):\n        self.data = data_frame\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Get file path and caption\n        image_info = self.data.iloc[idx]\n        image_path = image_info['filepath']  # Use the filepath column directly\n        caption = image_info['caption']\n\n        # Load and transform the image\n        image = Image.open(image_path).convert('RGB')  # Pastikan format PIL.Image\n       \n        return image, caption","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:58.055846Z","iopub.execute_input":"2024-12-18T12:30:58.056219Z","iopub.status.idle":"2024-12-18T12:30:58.06211Z","shell.execute_reply.started":"2024-12-18T12:30:58.056143Z","shell.execute_reply":"2024-12-18T12:30:58.061248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_size = 128","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:58.063322Z","iopub.execute_input":"2024-12-18T12:30:58.063662Z","iopub.status.idle":"2024-12-18T12:30:58.073203Z","shell.execute_reply.started":"2024-12-18T12:30:58.063622Z","shell.execute_reply":"2024-12-18T12:30:58.072472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transforms\n\nval_transform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Create datasets\ntrain_dataset = CustomImageCaptionDataset(\n    data_frame=train_data,\n\n)\n\nval_dataset = CustomImageCaptionDataset(\n    data_frame=val_data,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:58.074205Z","iopub.execute_input":"2024-12-18T12:30:58.07449Z","iopub.status.idle":"2024-12-18T12:30:58.08542Z","shell.execute_reply.started":"2024-12-18T12:30:58.074465Z","shell.execute_reply":"2024-12-18T12:30:58.084579Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:58.08654Z","iopub.execute_input":"2024-12-18T12:30:58.086913Z","iopub.status.idle":"2024-12-18T12:30:58.096661Z","shell.execute_reply.started":"2024-12-18T12:30:58.086875Z","shell.execute_reply":"2024-12-18T12:30:58.095886Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We'll use a pre-built Tokenizer for the BERT Model\n# https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:30:58.097676Z","iopub.execute_input":"2024-12-18T12:30:58.098013Z","iopub.status.idle":"2024-12-18T12:31:00.901265Z","shell.execute_reply.started":"2024-12-18T12:30:58.097976Z","shell.execute_reply":"2024-12-18T12:31:00.900543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    images, captions = zip(*batch)\n    # Tokenize dynamically\n    images, captions = zip(*batch)\n    return images, list(captions)  # Return captions as a list of raw strings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:00.902408Z","iopub.execute_input":"2024-12-18T12:31:00.902814Z","iopub.status.idle":"2024-12-18T12:31:00.908219Z","shell.execute_reply.started":"2024-12-18T12:31:00.902776Z","shell.execute_reply":"2024-12-18T12:31:00.907257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Loaders\ndata_loader_train = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n)\n\ndata_loader_val = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:00.909403Z","iopub.execute_input":"2024-12-18T12:31:00.909692Z","iopub.status.idle":"2024-12-18T12:31:01.093127Z","shell.execute_reply.started":"2024-12-18T12:31:00.909667Z","shell.execute_reply":"2024-12-18T12:31:01.092026Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataiter = next(iter(data_loader_val))\ntest_images, test_captions = dataiter","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.094576Z","iopub.execute_input":"2024-12-18T12:31:01.095008Z","iopub.status.idle":"2024-12-18T12:31:01.382112Z","shell.execute_reply.started":"2024-12-18T12:31:01.094966Z","shell.execute_reply":"2024-12-18T12:31:01.381303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the PIL images to tensors using ViTFeatureExtractor\ninputs = feature_extractor(images=[test_images[1]], return_tensors=\"pt\")  # Process a single image\n\n# Extract the image tensor\nimage_tensor = inputs[\"pixel_values\"][0]  # Shape: [C, H, W]\n\n# Visualize the image\nplt.figure(figsize=(3, 3))\nplt.imshow(image_tensor.permute(1, 2, 0).numpy())  # Permute to [H, W, C] for visualization\nplt.show()\n\n# Print the corresponding caption\ncaption = test_captions[1]\nprint(caption)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.383566Z","iopub.execute_input":"2024-12-18T12:31:01.38415Z","iopub.status.idle":"2024-12-18T12:31:01.64619Z","shell.execute_reply.started":"2024-12-18T12:31:01.384107Z","shell.execute_reply":"2024-12-18T12:31:01.645121Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.647428Z","iopub.execute_input":"2024-12-18T12:31:01.647718Z","iopub.status.idle":"2024-12-18T12:31:01.653533Z","shell.execute_reply.started":"2024-12-18T12:31:01.647687Z","shell.execute_reply":"2024-12-18T12:31:01.652689Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokens = tokenizer(test_captions, padding=True, truncation=True, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.654553Z","iopub.execute_input":"2024-12-18T12:31:01.654854Z","iopub.status.idle":"2024-12-18T12:31:01.667289Z","shell.execute_reply.started":"2024-12-18T12:31:01.654827Z","shell.execute_reply":"2024-12-18T12:31:01.666445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokens['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.668337Z","iopub.execute_input":"2024-12-18T12:31:01.668635Z","iopub.status.idle":"2024-12-18T12:31:01.678562Z","shell.execute_reply.started":"2024-12-18T12:31:01.668596Z","shell.execute_reply":"2024-12-18T12:31:01.677797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_ids = tokens['input_ids'][0]\ntokens['input_ids']","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.67991Z","iopub.execute_input":"2024-12-18T12:31:01.680161Z","iopub.status.idle":"2024-12-18T12:31:01.687937Z","shell.execute_reply.started":"2024-12-18T12:31:01.680137Z","shell.execute_reply":"2024-12-18T12:31:01.68711Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(token_ids))","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.69406Z","iopub.execute_input":"2024-12-18T12:31:01.694799Z","iopub.status.idle":"2024-12-18T12:31:01.699581Z","shell.execute_reply.started":"2024-12-18T12:31:01.694758Z","shell.execute_reply":"2024-12-18T12:31:01.698762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(1)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.700786Z","iopub.execute_input":"2024-12-18T12:31:01.701384Z","iopub.status.idle":"2024-12-18T12:31:01.710907Z","shell.execute_reply.started":"2024-12-18T12:31:01.701344Z","shell.execute_reply":"2024-12-18T12:31:01.709962Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TokenDrop(nn.Module):\n    \"\"\"For a batch of tokens indices, randomly replace a non-specical token.\n    \n    Args:\n        prob (float): probability of dropping a token\n        blank_token (int): index for the blank token\n        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n    \"\"\"\n\n    def __init__(self, prob=0.1, blank_token=103 , eos_token=102):\n        self.prob = prob\n        self.eos_token = eos_token\n        self.blank_token = blank_token\n\n    def __call__(self, sample):\n        # Randomly sample a bernoulli distribution with p=prob\n        # to create a mask where 1 means we will replace that token\n        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n        \n        # only replace if the token is not the eos token\n        can_drop = (~(sample == self.eos_token)).long()\n        mask = mask * can_drop\n        \n        # Do not replace the sos tokens\n        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n        \n        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n        \n        sample_out = (1 - mask) * sample + mask * replace_with\n        \n        return sample_out","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.712066Z","iopub.execute_input":"2024-12-18T12:31:01.712458Z","iopub.status.idle":"2024-12-18T12:31:01.719587Z","shell.execute_reply.started":"2024-12-18T12:31:01.712429Z","shell.execute_reply":"2024-12-18T12:31:01.718801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_patches(image_tensor, patch_size=16):\n    # Get the dimensions of the image tensor\n    bs, c, h, w = image_tensor.size()\n    \n    # Define the Unfold layer with appropriate parameters\n    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n    \n    # Apply Unfold to the image tensor\n    unfolded = unfold(image_tensor)\n    \n    # Reshape the unfolded tensor to match the desired output shape\n    # Output shape: BSxLxH, where L is the number of patches in each dimension\n    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n    \n    return unfolded\n\n# sinusoidal positional embeds\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n    \n    \n# Define a decoder module for the Transformer architecture\nclass Decoder(nn.Module):\n    def __init__(self, num_emb, hidden_size=768, num_layers=3, num_heads=4):\n        super(Decoder, self).__init__()\n\n        # Create an embedding layer for tokens\n        self.embedding = nn.Embedding(num_emb, hidden_size)\n\n        # Positional embeddings\n        self.pos_emb = SinusoidalPosEmb(hidden_size)\n\n        # Transformer decoder layers\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=hidden_size, nhead=num_heads,\n            dim_feedforward=hidden_size * 4, dropout=0.0,\n            batch_first=True\n        )\n        self.decoder_layers = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n\n        # Output layer\n        self.fc_out = nn.Linear(hidden_size, num_emb)\n\n    def forward(self, input_seq, encoder_output, input_padding_mask=None, encoder_padding_mask=None):\n        # Embedding and positional embeddings\n        input_embs = self.embedding(input_seq)\n        bs, l, h = input_embs.shape\n        pos_emb = self.pos_emb(torch.arange(l, device=input_seq.device)).reshape(1, l, h).expand(bs, l, h)\n        embs = input_embs + pos_emb\n    \n        # Handle optional padding mask\n        #if input_padding_mask is not None:\n            #print(\"Padding Mask Shape:\", input_padding_mask.shape)\n            #print(\"Padding Mask (Sample):\", input_padding_mask[0])\n    \n        # Causal mask\n        causal_mask = torch.triu(torch.ones(l, l, device=input_seq.device), 1).bool()\n        #print(\"Causal Mask Shape:\", causal_mask.shape)\n    \n        # Pass through transformer decoder layers\n        output = self.decoder_layers(\n            tgt=embs, memory=encoder_output, tgt_mask=causal_mask,\n            tgt_key_padding_mask=input_padding_mask, memory_key_padding_mask=encoder_padding_mask\n        )\n        return self.fc_out(output)\n\n\n    \n# Define an Vision Encoder-Decoder module for the Transformer architecture\nclass VisionEncoderDecoder(nn.Module):\n  \n    def __init__(self, encoder, decoder):\n        super(VisionEncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, input_image, target_seq, padding_mask):\n        # Use input_image directly (already preprocessed in the training loop)\n        encoder_outputs = self.encoder(**input_image).last_hidden_state\n    \n        # Ensure padding_mask is bool\n        padding_mask = padding_mask.bool()\n    \n        # Decode using the decoder\n        decoded_seq = self.decoder(input_seq=target_seq, encoder_output=encoder_outputs,\n                                   input_padding_mask=padding_mask)\n        return decoded_seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:01.720724Z","iopub.execute_input":"2024-12-18T12:31:01.721019Z","iopub.status.idle":"2024-12-18T12:31:01.735717Z","shell.execute_reply.started":"2024-12-18T12:31:01.720994Z","shell.execute_reply":"2024-12-18T12:31:01.734953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learning_rate = 1e-5","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.736772Z","iopub.execute_input":"2024-12-18T12:31:01.737118Z","iopub.status.idle":"2024-12-18T12:31:01.747472Z","shell.execute_reply.started":"2024-12-18T12:31:01.737093Z","shell.execute_reply":"2024-12-18T12:31:01.746751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available, set device accordingly\ndevice = torch.device(1 if torch.cuda.is_available() else 'cpu')\n\n# Embedding Size\nhidden_size = 768\n\n# Number of Transformer blocks for the (Encoder, Decoder)\nnum_layers = (6, 6)\n\n# MultiheadAttention Heads\nnum_heads = 8\n\n# Size of the patches\npatch_size = 8\n\n# Create model\ncaption_model = VisionEncoderDecoder(\n    encoder=vit_model,  # Ganti vit_model menjadi encoder\n    decoder=Decoder(num_emb=tokenizer.vocab_size, hidden_size=hidden_size,\n                    num_layers=num_layers[1], num_heads=num_heads)\n).to(device)\n\n# Initialize the optimizer with above parameters\noptimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\n\nscaler = torch.cuda.amp.GradScaler()\n\n# Define the loss function\nloss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n\ntd = TokenDrop(0.5)\n\n# Initialize the training loss logger\ntraining_loss_logger = []","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:01.748495Z","iopub.execute_input":"2024-12-18T12:31:01.748797Z","iopub.status.idle":"2024-12-18T12:31:02.923594Z","shell.execute_reply.started":"2024-12-18T12:31:01.74875Z","shell.execute_reply":"2024-12-18T12:31:02.922629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# See how many Parameters our Model has!\nnum_model_params = 0\nfor param in caption_model.parameters():\n    num_model_params += param.flatten().shape[0]\n\nprint(\"Number of Model Parameters : %d or >%d Juta Params!\" % (num_model_params, num_model_params//1e6))","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:02.924785Z","iopub.execute_input":"2024-12-18T12:31:02.925118Z","iopub.status.idle":"2024-12-18T12:31:02.934422Z","shell.execute_reply.started":"2024-12-18T12:31:02.925091Z","shell.execute_reply":"2024-12-18T12:31:02.933568Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nepochs = 50\ntraining_loss_logger = []\neval_loss_logger = []\neval_bleu_logger = []\neval_meteor_logger = []\neval_rouge_logger = []","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:31:02.93544Z","iopub.execute_input":"2024-12-18T12:31:02.9357Z","iopub.status.idle":"2024-12-18T12:31:02.941996Z","shell.execute_reply.started":"2024-12-18T12:31:02.935674Z","shell.execute_reply":"2024-12-18T12:31:02.94104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize ROUGE scorer\nrouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n\n# Initialize metric loggers\neval_bleu_scores = []\neval_meteor_scores = []\neval_rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:02.943112Z","iopub.execute_input":"2024-12-18T12:31:02.943363Z","iopub.status.idle":"2024-12-18T12:31:02.95049Z","shell.execute_reply.started":"2024-12-18T12:31:02.943338Z","shell.execute_reply":"2024-12-18T12:31:02.94977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(rouge_scores.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:02.951607Z","iopub.execute_input":"2024-12-18T12:31:02.952009Z","iopub.status.idle":"2024-12-18T12:31:02.959087Z","shell.execute_reply.started":"2024-12-18T12:31:02.95198Z","shell.execute_reply":"2024-12-18T12:31:02.958242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_length = 25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:02.96018Z","iopub.execute_input":"2024-12-18T12:31:02.960662Z","iopub.status.idle":"2024-12-18T12:31:02.967058Z","shell.execute_reply.started":"2024-12-18T12:31:02.960622Z","shell.execute_reply":"2024-12-18T12:31:02.966233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the EarlyStopping Class\nclass EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0.0, path='best_model.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.delta = delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.path = path\n        self.best_model_state = None\n\n    def __call__(self, current_loss, model):\n        if self.best_loss is None:\n            self.best_loss = current_loss\n            self.save_checkpoint(model)\n        elif current_loss > self.best_loss - self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = current_loss\n            self.save_checkpoint(model)\n            self.counter = 0\n\n    def save_checkpoint(self, model):\n        \"\"\"Saves model when validation loss decreases.\"\"\"\n        torch.save(model.state_dict(), self.path)\n        if self.verbose:\n            print(f'Validation loss decreased. Saving model to {self.path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:02.968137Z","iopub.execute_input":"2024-12-18T12:31:02.968837Z","iopub.status.idle":"2024-12-18T12:31:02.97614Z","shell.execute_reply.started":"2024-12-18T12:31:02.968795Z","shell.execute_reply":"2024-12-18T12:31:02.975394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Early Stopping with model checkpoint path\nearly_stopping = EarlyStopping(patience=5, verbose=True, path='best_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:02.977063Z","iopub.execute_input":"2024-12-18T12:31:02.977346Z","iopub.status.idle":"2024-12-18T12:31:02.989463Z","shell.execute_reply.started":"2024-12-18T12:31:02.977319Z","shell.execute_reply":"2024-12-18T12:31:02.988786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Length of Training Loss Logger:\", len(training_loss_logger))\nprint(\"Training Loss Logger Values:\", training_loss_logger[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:02.990408Z","iopub.execute_input":"2024-12-18T12:31:02.990696Z","iopub.status.idle":"2024-12-18T12:31:03.000303Z","shell.execute_reply.started":"2024-12-18T12:31:02.990661Z","shell.execute_reply":"2024-12-18T12:31:02.999489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Iterate over epochs\nfor epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n    # Set the model in training mode\n    caption_model.train()\n\n    # Track training loss for this epoch\n    epoch_train_loss = 0.0\n    num_train_batches = 0\n    \n    # Iterate over the training data loader\n    for images, captions in tqdm(data_loader_train, desc=\"Training\", leave=False):\n        # Preprocess images using ViTFeatureExtractor\n        inputs = feature_extractor(images=images, return_tensors=\"pt\")\n        inputs = {key: val.to(device) for key, val in inputs.items()}  # Send to GPU\n    \n        # Tokenize captions\n        # Tokenize captions\n        tokens = tokenizer(captions, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n        token_ids = tokens['input_ids'].to(device)\n        \n        # Convert attention mask to bool\n        padding_mask = (tokens['attention_mask'] == 0).to(device)\n    \n        # Shift target sequence\n        bs = token_ids.size(0)\n        target_ids = torch.cat((token_ids[:, 1:], torch.zeros(bs, 1, device=device).long()), 1)\n    \n        # Token drop augmentation\n        #tokens_in = td(token_ids)\n        tokens_in = token_ids\n\n        with torch.cuda.amp.autocast():\n            # Forward pass with preprocessed inputs\n            pred = caption_model(inputs, tokens_in, padding_mask=padding_mask)\n    \n        # Compute loss\n        pad_token_id = tokenizer.pad_token_id\n        loss_mask = (~(target_ids == pad_token_id)).float()\n        loss_vals = loss_fn(pred.transpose(1, 2), target_ids)\n\n        # Print intermediate debugging info\n        #print(\"Loss Values (sample):\", loss_vals[0, :10])\n        #print(\"Loss Mask (sample):\", loss_mask[0, :10])\n        #print(\"Loss Mask Sum:\", loss_mask.sum().item())\n        #print(\"Pred shape:\", pred.shape)\n        #print(\"Target IDs shape:\", target_ids.shape)\n        #print(\"Max target token:\", target_ids.max().item())\n        #print(\"Min target token:\", target_ids.min().item())\n        #print(\"Vocab size:\", tokenizer.vocab_size)\n\n        # Compute final loss\n        loss = (loss_vals * loss_mask).sum() / loss_mask.sum()\n        #print(\"Final Loss:\", loss.item())\n\n        # Backpropagation\n        optimizer.zero_grad()\n        #scaler.scale(loss).backward()\n        \n        # Accumulate loss for this epoch\n        epoch_train_loss += loss.item()\n        num_train_batches += 1\n        \n        #torch.nn.utils.clip_grad_norm_(caption_model.parameters(), max_norm=1.0)\n        \n        #scaler.step(optimizer)\n        #scaler.update()\n    \n        # For debugging, you can try without AMP and scaler to isolate issues:\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(caption_model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        # Log average training loss for the epoch\n        #avg_train_loss = epoch_train_loss / num_train_batches\n        #training_loss_logger.append(avg_train_loss)\n\n        # If the loss is NaN at this point, you can stop and inspect the printed values.\n        if torch.isnan(loss):\n            print(\"Encountered NaN loss!\")\n            break\n        \n    # Log average training loss for the epoch\n    avg_train_loss = epoch_train_loss / num_train_batches\n    training_loss_logger.append(avg_train_loss)\n    #print(f\"Epoch {epoch + 1}/{nepochs} - Avg Training Loss: {avg_train_loss:.4f}\")\n\n    \n    # Set the model in eval mode\n    caption_model.eval()\n    epoch_eval_loss = 0.0\n    num_eval_batches = 0\n    bleu_scores = []\n    meteor_scores = []\n    rouge1_scores = []\n    rouge2_scores = []\n    rougel_scores = []\n    \n    with torch.no_grad():\n        # Iterate over the training data loader\n        for images, captions in tqdm(data_loader_val, desc=\"Eval\", leave=False):\n\n            # Preprocess images using ViTFeatureExtractor\n            inputs = feature_extractor(images=images, return_tensors=\"pt\")\n            inputs = {key: val.to(device) for key, val in inputs.items()}  # Send to GPU\n        \n            # Tokenize captions\n            tokens = tokenizer(captions, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n            token_ids = tokens['input_ids'].to(device)\n            # Convert attention mask to boolean padding mask\n            padding_mask = (tokens['attention_mask'] == 0).to(device)\n\n            # Shift target sequence\n            bs = token_ids.size(0)\n            target_ids = torch.cat((token_ids[:, 1:], torch.zeros(bs, 1, device=device).long()), 1)\n            \n            with torch.amp.autocast(device_type='cuda'):\n                # Forward pass with preprocessed inputs\n                pred = caption_model(inputs, token_ids, padding_mask=padding_mask)\n\n            # Compute the loss\n            loss_mask = (~(target_ids == 0)).float()\n            loss = (loss_fn(pred.transpose(1, 2), target_ids) * loss_mask).sum() / loss_mask.sum()\n\n            # Accumulate validation loss for this epoch\n            epoch_eval_loss += loss.item()\n            num_eval_batches += 1\n            #eval_loss_logger.append(loss.item())\n\n            # Decode predictions\n            pred_ids = torch.argmax(pred, dim=-1)\n            predicted_captions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    \n            # Calculate metrics for each pair of prediction and ground truth\n            for pred_caption, gt_caption in zip(predicted_captions, captions):\n                # Tokenize ground truth and hypothesis\n                #reference = [gt_caption.split()]\n                #hypothesis = pred_caption.split()\n\n                reference = [tokenizer.tokenize(gt_caption.lower())]\n                hypothesis = tokenizer.tokenize(pred_caption.lower())\n\n                \n                # BLEU Score\n                smoothing_fn = SmoothingFunction().method1\n                bleu = sentence_bleu(reference, hypothesis, smoothing_function=smoothing_fn)\n                bleu_scores.append(bleu)\n    \n                # METEOR Score\n                meteor = meteor_score(reference, hypothesis)\n                meteor_scores.append(meteor)\n    \n                # ROUGE Scores\n                scores = rouge.score(gt_caption, pred_caption)\n                rouge1_scores.append(scores['rouge1'].fmeasure)\n                rouge2_scores.append(scores['rouge2'].fmeasure)\n                rougel_scores.append(scores['rougeL'].fmeasure)\n            \n            \n            # Log validation loss\n            #eval_loss_logger.append(loss.item())\n\n    # Log average evaluation loss for the epoch\n    avg_eval_loss = epoch_eval_loss / num_eval_batches if num_eval_batches > 0 else 0.0\n    eval_loss_logger.append(avg_eval_loss)\n\n\n    # Log average validation loss and BLEU score for the epoch\n    avg_eval_loss = epoch_eval_loss / num_eval_batches if num_eval_batches > 0 else 0.0\n    avg_bleu_score = np.mean(bleu_scores) if bleu_scores else 0.0\n    avg_meteor_score = np.mean(meteor_scores) if meteor_scores else 0.0\n    avg_rouge1 = np.mean(rouge1_scores) if rouge1_scores else 0.0\n    avg_rouge2 = np.mean(rouge2_scores) if rouge2_scores else 0.0\n    avg_rougeL = np.mean(rougel_scores) if rougel_scores else 0.0\n\n    #eval_loss_logger.append(avg_eval_loss)\n    eval_bleu_logger.append(avg_bleu_score)\n    eval_meteor_logger.append(avg_meteor_score)\n    eval_rouge_logger.append({'rouge1': avg_rouge1, 'rouge2': avg_rouge2, 'rougeL': avg_rougeL})\n    \n    print(f\"Epoch {epoch + 1}/{nepochs} - Avg Eval Loss: {avg_eval_loss:.4f} - \"\n          f\"Avg BLEU: {avg_bleu_score:.4f} - Avg Meteor: {avg_meteor_score:.4f} - \"\n          f\"Avg Rouge1: {avg_rouge1:.4f}, Rouge2: {avg_rouge2:.4f}, RougeL: {avg_rougeL:.4f}\")\n\n    # Early Stopping Check\n    early_stopping(avg_eval_loss, caption_model)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered. Restoring the best model.\")\n        caption_model.load_state_dict(torch.load('best_model.pt'))\n        break\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:31:03.001431Z","iopub.execute_input":"2024-12-18T12:31:03.001782Z","iopub.status.idle":"2024-12-18T13:45:35.051695Z","shell.execute_reply.started":"2024-12-18T12:31:03.001721Z","shell.execute_reply":"2024-12-18T13:45:35.050793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model's state dict\ntorch.save(caption_model.state_dict(), \"caption_model_state_dict.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:35.053153Z","iopub.execute_input":"2024-12-18T13:45:35.053771Z","iopub.status.idle":"2024-12-18T13:45:36.134586Z","shell.execute_reply.started":"2024-12-18T13:45:35.053708Z","shell.execute_reply":"2024-12-18T13:45:36.133867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model and optimizer state dicts\ntorch.save({\n    'epoch': nepochs,\n    'model_state_dict': caption_model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': training_loss_logger[-1],\n}, \"checkpoint.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:36.135692Z","iopub.execute_input":"2024-12-18T13:45:36.135977Z","iopub.status.idle":"2024-12-18T13:45:39.436837Z","shell.execute_reply.started":"2024-12-18T13:45:36.135951Z","shell.execute_reply":"2024-12-18T13:45:39.435746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'checkpoint.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:57:48.607447Z","iopub.execute_input":"2024-12-18T13:57:48.60813Z","iopub.status.idle":"2024-12-18T13:57:48.614255Z","shell.execute_reply.started":"2024-12-18T13:57:48.608092Z","shell.execute_reply":"2024-12-18T13:57:48.6132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#eval_metrics_logger = {\n    #\"BLEU\": avg_bleu,\n    #\"METEOR\": avg_meteor,\n    #\"ROUGE-1\": avg_rouge_1,\n    #\"ROUGE-2\": avg_rouge_2,\n    #\"ROUGE-L\": avg_rouge_l\n#}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:39.447961Z","iopub.execute_input":"2024-12-18T13:45:39.448638Z","iopub.status.idle":"2024-12-18T13:45:39.520882Z","shell.execute_reply.started":"2024-12-18T13:45:39.448596Z","shell.execute_reply":"2024-12-18T13:45:39.519897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Sample caption:\", captions[0])\nprint(\"Decoded:\", tokenizer.decode(token_ids[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:39.522216Z","iopub.execute_input":"2024-12-18T13:45:39.523165Z","iopub.status.idle":"2024-12-18T13:45:39.789053Z","shell.execute_reply.started":"2024-12-18T13:45:39.523122Z","shell.execute_reply":"2024-12-18T13:45:39.787902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(training_loss_logger), len(eval_loss_logger), len(eval_bleu_logger))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:39.790241Z","iopub.execute_input":"2024-12-18T13:45:39.790514Z","iopub.status.idle":"2024-12-18T13:45:40.167635Z","shell.execute_reply.started":"2024-12-18T13:45:39.790489Z","shell.execute_reply":"2024-12-18T13:45:40.166319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:40.169136Z","iopub.execute_input":"2024-12-18T13:45:40.169553Z","iopub.status.idle":"2024-12-18T13:45:41.613704Z","shell.execute_reply.started":"2024-12-18T13:45:40.169504Z","shell.execute_reply":"2024-12-18T13:45:41.612794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training + Eval Loss","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assume avg_training_loss_logger and avg_eval_loss_logger contain epoch-level losses\navg_training_loss_logger = training_loss_logger  # Replace with epoch-wise averaged training loss\navg_eval_loss_logger = eval_loss_logger          # Replace with epoch-wise averaged evaluation loss\n\n# Define the range and step size for y-axis ticks\ny_min, y_max = min(min(avg_training_loss_logger), min(avg_eval_loss_logger)), \\\n               max(max(avg_training_loss_logger), max(avg_eval_loss_logger))\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks (adjust if needed)\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\nplt.figure(figsize=(10, 5))\n\n# Plot average training loss per epoch\nplt.plot(range(1, len(avg_training_loss_logger) + 1), avg_training_loss_logger, \n         label=\"Training Loss\", color='tab:blue')\n\n# Plot average evaluation loss per epoch\nplt.plot(range(1, len(avg_eval_loss_logger) + 1), avg_eval_loss_logger, \n         label=\"Evaluation Loss\", color='tab:orange')\n\n# Apply custom y-ticks\nplt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\nplt.title(\"Training and Evaluation Loss per Epoch\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:41.614833Z","iopub.execute_input":"2024-12-18T13:45:41.615152Z","iopub.status.idle":"2024-12-18T13:45:41.91617Z","shell.execute_reply.started":"2024-12-18T13:45:41.615102Z","shell.execute_reply":"2024-12-18T13:45:41.915136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the range and step size for y-axis ticks for both training and eval losses\ny_min, y_max = min(min(training_loss_logger), min(eval_loss_logger)), max(max(training_loss_logger), max(eval_loss_logger))\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks (adjust if needed)\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\n_ = plt.figure(figsize=(10, 5))\n\n# Plot training loss\n_ = plt.plot(training_loss_logger, label=\"Training Loss\", color='tab:blue')\n\n# Plot evaluation loss\n_ = plt.plot(eval_loss_logger, label=\"Evaluation Loss\", color='tab:orange')\n\n# Apply custom y-ticks\n_ = plt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\n_ = plt.title(\"Training and Evaluation Loss\")\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"Loss\")\n_ = plt.legend()\n_ = plt.grid(True)\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:41.916996Z","iopub.execute_input":"2024-12-18T13:45:41.917236Z","iopub.status.idle":"2024-12-18T13:45:42.215391Z","shell.execute_reply.started":"2024-12-18T13:45:41.917213Z","shell.execute_reply":"2024-12-18T13:45:42.214456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bleu Score","metadata":{}},{"cell_type":"code","source":"# Define the range and step size for y-axis ticks\ny_min, y_max = min(eval_bleu_logger), max(eval_bleu_logger)\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks (adjust if needed)\ny_ticks = np.arange(y_min, y_max + step, step)\n\n_ = plt.figure(figsize=(10, 5))\n_ = plt.plot(eval_bleu_logger[:])\n_ = plt.title(\"Bleu Score\")\n\n# Apply custom y-ticks with fewer steps\n_ = plt.yticks(y_ticks)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:42.216806Z","iopub.execute_input":"2024-12-18T13:45:42.217473Z","iopub.status.idle":"2024-12-18T13:45:42.481864Z","shell.execute_reply.started":"2024-12-18T13:45:42.21743Z","shell.execute_reply":"2024-12-18T13:45:42.480948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bleu + METEOR Score","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the range and step size for y-axis ticks for both training and eval losses\ny_min, y_max = min(min(eval_bleu_logger), min(eval_meteor_logger)), max(max(eval_bleu_logger), max(eval_meteor_logger))\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks (adjust if needed)\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\n_ = plt.figure(figsize=(10, 5))\n\n# Plot training loss\n_ = plt.plot(eval_bleu_logger, label=\"Bleu\", color='tab:blue')\n\n# Plot evaluation loss\n_ = plt.plot(eval_meteor_logger, label=\"Meteor\", color='tab:orange')\n\n# Apply custom y-ticks\n_ = plt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\n_ = plt.title(\"Bleu and Meteor Value\")\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"Loss\")\n_ = plt.legend()\n_ = plt.grid(True)\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:42.482981Z","iopub.execute_input":"2024-12-18T13:45:42.483262Z","iopub.status.idle":"2024-12-18T13:45:42.707962Z","shell.execute_reply.started":"2024-12-18T13:45:42.483237Z","shell.execute_reply":"2024-12-18T13:45:42.707069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract ROUGE scores\nrouge1_scores = [entry['rouge1'] for entry in eval_rouge_logger]\nrouge2_scores = [entry['rouge2'] for entry in eval_rouge_logger]\nrougeL_scores = [entry['rougeL'] for entry in eval_rouge_logger]\n\n# Define the range and step size for y-axis ticks for ROUGE scores\nall_rouge_scores = rouge1_scores + rouge2_scores + rougeL_scores\ny_min, y_max = min(all_rouge_scores), max(all_rouge_scores)\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\n_ = plt.figure(figsize=(10, 5))\n\n# Plot ROUGE scores\n_ = plt.plot(rouge1_scores, label=\"ROUGE-1\", color='tab:green')\n_ = plt.plot(rouge2_scores, label=\"ROUGE-2\", color='tab:red')\n_ = plt.plot(rougeL_scores, label=\"ROUGE-L\", color='tab:purple')\n\n# Apply custom y-ticks\n_ = plt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\n_ = plt.title(\"ROUGE Scores Across Epochs\")\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"ROUGE Score\")\n_ = plt.legend()\n_ = plt.grid(True)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:00:35.65428Z","iopub.execute_input":"2024-12-18T14:00:35.654961Z","iopub.status.idle":"2024-12-18T14:00:35.965453Z","shell.execute_reply.started":"2024-12-18T14:00:35.654923Z","shell.execute_reply":"2024-12-18T14:00:35.964497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract ROUGE scores\nrouge1_scores = [entry['rouge1'] for entry in eval_rouge_logger]\nrouge2_scores = [entry['rouge2'] for entry in eval_rouge_logger]\nrougeL_scores = [entry['rougeL'] for entry in eval_rouge_logger]\n\n# Define the range and step size for y-axis ticks for all metrics\nall_metrics = eval_bleu_logger + eval_meteor_logger + rouge1_scores + rouge2_scores + rougeL_scores\ny_min, y_max = min(all_metrics), max(all_metrics)\nstep = (y_max - y_min) / 10  # Set 10 evenly spaced ticks\ny_ticks = np.arange(y_min, y_max + step, step)\n\n# Create the plot\n_ = plt.figure(figsize=(12, 6))\n\n# Plot BLEU and Meteor scores\n_ = plt.plot(eval_bleu_logger, label=\"BLEU\", color='tab:blue')\n_ = plt.plot(eval_meteor_logger, label=\"Meteor\", color='tab:orange')\n\n# Plot ROUGE scores\n_ = plt.plot(rouge1_scores, label=\"ROUGE-1\", color='tab:green')\n_ = plt.plot(rouge2_scores, label=\"ROUGE-2\", color='tab:red')\n_ = plt.plot(rougeL_scores, label=\"ROUGE-L\", color='tab:purple')\n\n# Apply custom y-ticks\n_ = plt.yticks(y_ticks)\n\n# Add labels, title, legend, and grid\n_ = plt.title(\"Evaluation Metrics Across Epochs\")\n_ = plt.xlabel(\"Epochs\")\n_ = plt.ylabel(\"Metric Value\")\n_ = plt.legend()\n_ = plt.grid(True)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:02:29.019812Z","iopub.execute_input":"2024-12-18T14:02:29.020448Z","iopub.status.idle":"2024-12-18T14:02:29.346292Z","shell.execute_reply.started":"2024-12-18T14:02:29.020413Z","shell.execute_reply":"2024-12-18T14:02:29.345343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a dataloader itterable object\ndataiter = next(iter(data_loader_val))\n# Sample from the itterable object\ntest_images, test_captions = dataiter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:45:42.709104Z","iopub.execute_input":"2024-12-18T13:45:42.709369Z","iopub.status.idle":"2024-12-18T13:45:42.802445Z","shell.execute_reply.started":"2024-12-18T13:45:42.709344Z","shell.execute_reply":"2024-12-18T13:45:42.801798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose an index within the batch\nindex = 5\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Sesuaikan ukuran jika perlu\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_image = transform(test_images[index]).unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:55:53.837354Z","iopub.execute_input":"2024-12-18T13:55:53.837686Z","iopub.status.idle":"2024-12-18T13:55:53.844693Z","shell.execute_reply.started":"2024-12-18T13:55:53.837655Z","shell.execute_reply":"2024-12-18T13:55:53.843796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets visualise an entire batch of images!\nplt.figure(figsize = (3,3))\nout = torchvision.utils.make_grid(test_image, 1, normalize=True)\n_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\nprint(test_captions[index])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:55:55.598035Z","iopub.execute_input":"2024-12-18T13:55:55.598875Z","iopub.status.idle":"2024-12-18T13:55:55.844468Z","shell.execute_reply.started":"2024-12-18T13:55:55.598839Z","shell.execute_reply":"2024-12-18T13:55:55.843572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add the Start-Of-Sentence token to the prompt to signal the network to start generating the caption\nsos_token = 101 * torch.ones(1, 1).long()\n\n# Set the temperature for sampling during generation\ntemp = 0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:55:58.325763Z","iopub.execute_input":"2024-12-18T13:55:58.326083Z","iopub.status.idle":"2024-12-18T13:55:58.33097Z","shell.execute_reply.started":"2024-12-18T13:55:58.32604Z","shell.execute_reply":"2024-12-18T13:55:58.329955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_tokens = [sos_token]\ncaption_model.eval()\n\nwith torch.no_grad():\n    # Encode the input image\n    with torch.amp.autocast(device_type='cuda'):\n        # Forward pass\n        image_embedding = caption_model.encoder(test_image.to(device)).last_hidden_state\n\n    # Generate the answer tokens\n    for i in range(50):\n        input_tokens = torch.cat(log_tokens, 1)\n        \n        # Decode the input tokens into the next predicted tokens\n        data_pred = caption_model.decoder(input_tokens.to(device), image_embedding)\n        \n        # Sample from the distribution of predicted probabilities\n        dist = Categorical(logits=data_pred[:, -1] / temp)\n        next_tokens = dist.sample().reshape(1, 1)\n        \n        # Append the next predicted token to the sequence\n        log_tokens.append(next_tokens.cpu())\n        \n        # Break the loop if the End-Of-Caption token is predicted\n        if next_tokens.item() == 102:\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:56:01.091308Z","iopub.execute_input":"2024-12-18T13:56:01.091974Z","iopub.status.idle":"2024-12-18T13:56:01.167221Z","shell.execute_reply.started":"2024-12-18T13:56:01.091939Z","shell.execute_reply":"2024-12-18T13:56:01.166389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the list of token indices to a tensor\npred_text = torch.cat(log_tokens, 1)\n\n# Convert the token indices to their corresponding strings using the vocabulary\npred_text_strings = tokenizer.decode(pred_text[0], skip_special_tokens=True)\n\n# Join the token strings to form the predicted text\npred_text = \"\".join(pred_text_strings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:56:01.168656Z","iopub.execute_input":"2024-12-18T13:56:01.168942Z","iopub.status.idle":"2024-12-18T13:56:01.173915Z","shell.execute_reply.started":"2024-12-18T13:56:01.168917Z","shell.execute_reply":"2024-12-18T13:56:01.172926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets visualise an entire batch of images!\nplt.figure(figsize = (3, 3))\nout = torchvision.utils.make_grid(test_image, 1, normalize=True)\n_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n\n# Print the predicted text\nprint(pred_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:56:01.188225Z","iopub.execute_input":"2024-12-18T13:56:01.189027Z","iopub.status.idle":"2024-12-18T13:56:01.448413Z","shell.execute_reply.started":"2024-12-18T13:56:01.188971Z","shell.execute_reply":"2024-12-18T13:56:01.447487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(caption_model, \"/kaggle/working/ViT-Transformer_Decoder-BERT_Word_Embbed_V2.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:56:01.44994Z","iopub.execute_input":"2024-12-18T13:56:01.450209Z","iopub.status.idle":"2024-12-18T13:56:03.305889Z","shell.execute_reply.started":"2024-12-18T13:56:01.450183Z","shell.execute_reply":"2024-12-18T13:56:03.304783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference On Best Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, ViTFeatureExtractor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# ===== 1. Load the Model ===== #\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the model architecture\ncaption_model = VisionEncoderDecoder(\n    encoder=vit_model,  # Ganti vit_model menjadi encoder\n    decoder=Decoder(num_emb=tokenizer.vocab_size, hidden_size=hidden_size,\n                    num_layers=num_layers[1], num_heads=num_heads)\n).to(device)\n\ncaption_model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\ncaption_model.eval()\n#\n\n# Load the tokenizer and feature extractor\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Replace if custom tokenizer\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n\n# ===== 2. User Input: Load New Image ===== #\ndef load_and_preprocess_image(image_path):\n    \"\"\"\n    Load an image and preprocess it for the model.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    return inputs[\"pixel_values\"].to(device), image\n\n# Prompt user for image input in Kaggle\nimage_path = input(\"Please provide the path to the new image: \")  # Example: \"../input/my-image.jpg\"\nimage_tensor, original_image = load_and_preprocess_image(image_path)\n\n# ===== 3. Generate Caption ===== #\ndef generate_caption(model, image_tensor, tokenizer, max_length=30, temp=0.7):\n    \"\"\"\n    Generate a caption for the input image using the trained model.\n    \"\"\"\n    sos_token = torch.tensor([[tokenizer.cls_token_id]]).to(device)  # Start token\n    tokens = [sos_token]\n\n    with torch.no_grad():\n        # Get image features using encoder\n        image_embedding = model.encoder(pixel_values=image_tensor).last_hidden_state\n\n        for _ in range(max_length):\n            input_tokens = torch.cat(tokens, dim=1)  # Concatenate tokens\n            outputs = model.decoder(input_seq=input_tokens, encoder_output=image_embedding)\n\n            # Sample the next token with temperature\n            logits = outputs[:, -1] / temp  # Use logits of last token\n            next_token = torch.argmax(logits, dim=-1).unsqueeze(1)\n            \n            # Append next token and stop if end token is generated\n            tokens.append(next_token)\n            if next_token.item() == tokenizer.sep_token_id:  # End token\n                break\n\n    # Decode generated tokens\n    caption = tokenizer.decode(torch.cat(tokens, dim=1)[0], skip_special_tokens=True)\n    return caption\n\n# Generate the caption\ncaption = generate_caption(caption_model, image_tensor, tokenizer)\n\n# ===== 4. Display Image and Caption ===== #\nplt.imshow(original_image)\nplt.axis(\"off\")\nplt.title(f\"Generated Caption: {caption}\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:25:55.089636Z","iopub.execute_input":"2024-12-18T14:25:55.090291Z","iopub.status.idle":"2024-12-18T14:26:06.898745Z","shell.execute_reply.started":"2024-12-18T14:25:55.090253Z","shell.execute_reply":"2024-12-18T14:26:06.897939Z"}},"outputs":[],"execution_count":null}]}